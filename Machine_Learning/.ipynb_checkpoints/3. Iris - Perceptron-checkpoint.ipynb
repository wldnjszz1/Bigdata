{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습용 데이터\n",
    "from sklearn import datasets\n",
    "# 시각화 라이브러리\n",
    "import matplotlib.pyplot as plt\n",
    "# 데이터를 학습용과 테스트용으로 나눌 수 있는 함수\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 표준화 작업을 위한 모듈\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 사용할 모델\n",
    "from sklearn.linear_model import Perceptron\n",
    "# 정확도 계산을 위한 함수\n",
    "from sklearn.metrics import accuracy_score\n",
    "# 파일저장\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 준비하는 과정\n",
    "def step1_get_data():\n",
    "    # 데이터 추출\n",
    "    iris = datasets.load_iris()\n",
    "    X = iris.data[:100, [2,3]] # 모든 row, index 2, 3인 columns만 가져오기\n",
    "    y = iris.target[:100]\n",
    "    names = iris.target_names[:2]\n",
    "    \n",
    "    return X, y, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분석\n",
    "def step2_data_analysis(X, y, names):\n",
    "    # 시각화를 한다\n",
    "    plt.scatter(X[:50, 0], X[:50, 1]) # 가로길이가 x축 , 너비가 y축 \n",
    "    plt.scatter(X[50:100, 0], X[50:100, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 학습\n",
    "def step3_learn(X, y):\n",
    "    # 비어 있는 곳을 채워준다\n",
    "    # 이상데이터에 대한 처리\n",
    "    # 문자열 데이터를 수치화\n",
    "    # 데이터의 표준화 작업\n",
    "    \n",
    "    # 데이터를 7:3 비율로 나눈다\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3) # train은 연습시키는 것, test는 테스트 해보기\n",
    "    \n",
    "    # 표준화 작업을 위한 객체를 추출한다\n",
    "    # 표준화 작업 : 데이터를 표준 정규 분포로 변환하여 적은 학습 횟수와 높은 학습 정확도를 갖기 위해 하는 작업\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    # 데이터를 표준화한다\n",
    "    sc.fit(X_train)\n",
    "    X_train_std = sc.transform(X_train)\n",
    "    \n",
    "    # 학습한다\n",
    "    model = Perceptron(eta0 = 0.01, max_iter = 40)\n",
    "    model.fit(X_train_std, y_train)\n",
    "    \n",
    "    # 정확도를 확인\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    y_pred = model.predict(X_test_std)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    print(f'학습 정확도 : {score}')\n",
    "    \n",
    "    with open('model2.dat', 'wb') as fp : \n",
    "        pickle.dump(sc, fp)\n",
    "        pickle.dump(model, fp)\n",
    "    print('모델 저장 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y, names = step1_get_data()\n",
    "step2_data_analysis(X, y, names)\n",
    "step3_learn(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 객체 복원\n",
    "with open('model2.dat', 'rb') as fp :\n",
    "    sc = pickle.load(fp)\n",
    "    model = pickle.load(fp)\n",
    "    \n",
    "a1 = input('꽃잎의 너비를 입력하세요 : ')\n",
    "a1 = input('꽃잎의 길이를 입력하세요 : ')\n",
    "\n",
    "X = np.array([[float(a1), float(a2)]])\n",
    "X_std = sc.transform(X)\n",
    "\n",
    "# 예측 결과를 가져온다\n",
    "y = model.predict(X_std)\n",
    "if y[0] == 0:\n",
    "    print('Iris-setosa')\n",
    "else:\n",
    "    print('Iris-versicolor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
